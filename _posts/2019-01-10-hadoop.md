---
title: 学习用的hadoop容器（上）
abstract: hadoop的学习容器制作与使用全过程。
---

## {{ page.title }}


（本文比较长，而且container还在调试当中，所以决定拆成三篇文章来发，边做边发。）

制作了一个`hadoop-learn`的container用来学习Hadoop：

```bash
$ docker ps
CONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS              PORTS                                                                                                                                NAMES
898e67a47ea7        sequenceiq/hadoop-docker:2.7.1   "/etc/bootstrap.sh -…"   15 hours ago        Up 14 minutes       2122/tcp, 8030-8033/tcp, 8040/tcp, 8042/tcp, 8088/tcp, 19888/tcp, 49707/tcp, 50010/tcp, 50020/tcp, 50070/tcp, 50075/tcp, 50090/tcp   hadoop-learn
```

这个容器是基于一个github上面的hadoop容器：

- [GitHub - tomwhite/hadoop-book: Example source code accompanying O’Reilly’s “Hadoop: The Definitive Guide” by Tom White](https://github.com/tomwhite/hadoop-book)

上面这个容器已经安装配置好了一个可以直接用起来的，`pseudo-distributed`模式的hadoop instance。

按照上面网站的文档，下载并启动登录进这个容器，就可以直接使用起来hadoop。但是它这个项目已经好久没有更新了，里面有一些软件包需要安装，有一些配置需要调整，因此我在这个container的基础上做了新的container，在这里：

- [Hadoop Learn Container](https://cloud.docker.com/repository/docker/weli/hadoop-learn)

这个container里面的内容，下面详细展开讲。此外，未来我会把这个container做成Dockerfile，放在github上面，方便日后维护和更新。后续的工作我会放在这个fork仓库里面：

- [GitHub - liweinan/hadoop-docker: Hadoop docker image](https://github.com/liweinan/hadoop-docker)

回到这个container本身的话题。第一次下载，启动，登录进这个容器，使用下面的命令：

```bash
$ docker run -it weli/hadoop-learn sh
```

下载并启动完成后，就可以看到已经登录进了容器。此时我们在主机上执行`docker ps`命令，可以查看已经启动的容器：

```bash
$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                                                                                                                NAMES
5660bba0258f        weli/hadoop-learn   "sh"                3 minutes ago       Up 3 minutes        2122/tcp, 8030-8033/tcp, 8040/tcp, 8042/tcp, 8088/tcp, 19888/tcp, 49707/tcp, 50010/tcp, 50020/tcp, 50070/tcp, 50075/tcp, 50090/tcp   tender_borg
```

为了以后方便找到这个container，可以把他重新命名：

```bash
$ docker rename tender_borg my_hadoop_learn_container
```

这样，这个容器的名称就被修改好了：

![](https://raw.githubusercontent.com/liweinan/blogpicbackup/master/data/75C2E52D-C9C9-4F2E-9BAA-7022BC515245.png)

我们可以停掉这个容器：

```bash
$ docker stop my_hadoop_learn_container
my_hadoop_learn_container
```

然后以后想使用的时候，再启动这个容器：

```bash
$ docker start my_hadoop_learn_container
my_hadoop_learn_container
```

然后登录进这个容器：

```bash
$ docker exec -it my_hadoop_learn_container sh
$ 
```

这样，以后在这个容器里面的进度，就都在里面了，而不要每次都创建一个新的容器。如果想本地保存这个容器，就使用`docker commit`命令：

```bash
$ docker commit my_hadoop_learn_container
sha256:89cc66f16a6288910fbe8483d24fa528339ac6b66032793f8b4c5b1e3ef071b7
```

这样，这个容器的内容就被保存成了image。可以通过`docker image`命令来查看：

```bash
$ docker image ls
REPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE
<none>                     <none>              89cc66f16a62        34 seconds ago      2.99GB
```

可以看到刚刚保存的这个image，我们还可以通过`docker save`命令把它拷贝成文件，或者上传到dockerhub。docker的基本使用不是本文的重点，就不详细展开了。

回到这个容器的话题，登录进容器以后，首先进入root的用户目录，并且执行初始化脚本：

```bash
$ cd
$ pwd
/root
$ . ~/.bashrc
```

这个`.bashrc`里面配置好了sdkman的初始化脚本：

```bash
$ cat ~/.bashrc

#THIS MUST BE AT THE END OF THE FILE FOR SDKMAN TO WORK!!!
export SDKMAN_DIR="/root/.sdkman"
[[ -s "/root/.sdkman/bin/sdkman-init.sh" ]] && source "/root/.sdkman/bin/sdkman-init.sh"
```

这个容器里面已经安装了sdkman：

- [The Software Development Kit Manager](https://sdkman.io/)

`sdkman`是一个java的包管理工具，可以用它安装和管理各种版本的java和java工具。这个容器里面使用它安装了`maven`：

```bash
$ which mvn
/root/.sdkman/candidates/maven/current/bin/mvn
```

因为这个容器里安装了`Hadoop Definitive Guide`的源代码，位于`/root/hadoop-book`这个目录：

![](https://raw.githubusercontent.com/liweinan/blogpicbackup/master/data/55E50631-DE9D-4618-BCB7-5F254CE67DF8.png)

并且已经用maven编译好了：

![](https://raw.githubusercontent.com/liweinan/blogpicbackup/master/data/C3395662-8EE7-4EFA-B0DC-BAA4A69B00E8.png)

方便大家学习使用。

此外，这个容器使用的centos版本有点儿老，所以里面已经更新了centos的所有的软件包。

首先查看一下hadoop的环境变量：

```bash
$ env | grep HADOOP
HADOOP_PREFIX=/usr/local/hadoop
HADOOP_HDFS_HOME=/usr/local/hadoop
HADOOP_COMMON_HOME=/usr/local/hadoop
HADOOP_YARN_HOME=/usr/local/hadoop
HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
HADOOP_MAPRED_HOME=/usr/local/hadoop
```

然后进到hadoop的安装目录，停掉hadoop的`pseudo-distributed`模式：

```bash
$ pwd
/usr/local/hadoop
$ cd sbin/
$ ./stop-all.sh
This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh
19/01/06 21:02:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Stopping namenodes on [5660bba0258f]
5660bba0258f: stopping namenode
localhost: stopping datanode
Stopping secondary namenodes [0.0.0.0]
0.0.0.0: stopping secondarynamenode
19/01/06 21:03:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
stopping yarn daemons
stopping resourcemanager
localhost: stopping nodemanager
no proxyserver to stop
```

之所以要停掉这个模式，是因为这个容器里面还有一些配置没有搞好，等搞好以后，我会更新container，并且再发下一篇文章进行讲解。

∎
