---
title: Using eGPU and Tensorflow on MacOS
---

MacOS starts to support Tensorflow officially recently, and here is the project link:

* [Mac-optimized TensorFlow and TensorFlow Addons](https://github.com/apple/tensorflow_macos)

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/CD215399-D4E0-44D2-B1FE-E0902DEDBB8C.png)

From the above project introduction we can see that it uses its `ML Computer` framework to support hardware-accelerated `Tensorflow` natively. I just have a eGPU by my side:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/6101621836182_.pic.jpg)

It’s a `BlackMagic` eGPU box and it contains an `AMD Radeon Pro 580` card inside:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/AFA14B69-9DD5-4C3D-B657-D6CC9FF1F980.png)

So I followed the installation instruction on above `tensorflow_macos` GitHub project page and setup my local environment for use. Here is part of the installation process:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/74781621681432_.pic_hd.jpg)

After installed it at my working directory `~/tensorflow_macos_venv/`, I entered the virtual environment:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/74801621681496_.pic.jpg)

In above virtual environment, I installed `jupyter` for coding and start `jupyter-notebook` to enter some test code:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/76461621742927_.pic.jpg)

From above code we can see that the tensorflow version is `2.4.0-rc0`. Then I used the code here for testing:

- [TensorFlow 2 quickstart for experts](https://www.tensorflow.org/tutorials/quickstart/advanced)

And here is the testing result:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/6DFC9A74-33D0-4C8F-972C-69DBCA7640F7.png)

During the code running process, I can see that the code is actually running on my eGPU:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/76361621742663_.pic.jpg)

Which means MacOS is now officially supporting hardware accelerated Machine Learning framework.

To see the difference on running performance on GPU vs. CPU, we can use this project for demonstration:

- [GitHub - plaidml/plaidml: PlaidML is a framework for making deep learning work everywhere.](https://github.com/plaidml/plaidml)

The advantage of this project is that you can select a hardware for running your Machine Learning code. It supports several ML frameworks out-of-box:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/00CE47D4-2DA1-4E5E-96BF-C2EAD108233F.png)

After installing the above framework and setup the virtual environment properly. I run the `plaid-setup` command it provides to select the hardware to use for running ML code:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/74871621687581_.pic.jpg)

As we can see there are several hardwares I can select on my computer. First one is by default CPU, and others are my integrated Intel graphic card and outer eGPU AMD card.

Moreover, for the eGPU, there are two options to select, one is to use the `OpenCL` layer and the other is to use the bare metal support of hardware.

After selecting devices one by one, I run the `plaidbench keras mobilenet` command to testing the performance with its `Keras` support running `mobilenet` model.

Here are the results of all the devices one by one:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/75941621739658_.pic.jpg)

From above the first result is running on bare metal external GPU card, which the tests used around 8 seconds to finish. Then it’s the result running on same external AMD card running with OpenCL layer:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/75951621739659_.pic.jpg)

From above we can see that it used more time running the test, which is around 12 seconds. The third test is running on integrated intel display card:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/75851621739368_.pic.jpg)

It used around 22 seconds finished testing. The last one is to run the test directly on the CPU:

![](https://raw.githubusercontent.com/liweinan/blogpic2021i/master/may24/75861621739369_.pic.jpg)

We can see the result is dramatically slower than using a GPU. It is around 110 seconds finish tests, which is 10 times more slower than running on bare metal eGPU.

In addition, here is a list of the eGPU devices Apple currently supports:

- [Use an external graphics processor with your Mac - Apple Support](https://support.apple.com/en-us/HT208544)

And as Apple is migrating to Arm structure, we may see the potential that using MacOS as the machine learning platform in future.


